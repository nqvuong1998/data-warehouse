spark:
  spark_conf:
    spark.executor.memory: 2g
    spark.executor.cores: 1
    spark.driver.memory: 1g
    spark.cores.max: 3
    spark.sql.caseSensitive: true
    spark.default.parallelism: 6
    spark.sql.shuffle.partitions: 6
    spark.sql.sources.partitionOverwriteMode: dynamic
    spark.serializer: org.apache.spark.serializer.KryoSerializer
  hadoop_conf:
    fs.s3a.endpoint: https://s3.ap-southeast-1.amazonaws.com

java:
  java_conf:
    user.timezone: Asia/Ho_Chi_Minh

mart:
  source:
    daily_account_closing_balance_mart:
      format: org.apache.hudi
      url: ""
      path: /Users/lap14151/Downloads/Code/data-warehouse/data/warehouse/daily_account_closing_balance_mart/*
      schema:
        type: struct
        field:
          date_str: string
          account_skey: long
          account_number: int
          closing_balance: double
          updated_time: timestamp
          sys_partition: string

  destination:
    format: org.apache.hudi
    url: ""
    path: /Users/lap14151/Downloads/Code/data-warehouse/data/warehouse/monthly_account_average_balance_mart/
    mode: append
    option:
      hoodie.insert.shuffle.parallelism: 2
      hoodie.upsert.shuffle.parallelism: 2
      hoodie.delete.shuffle.parallelism: 2
      hoodie.bulkinsert.shuffle.parallelism: 2
      hoodie.datasource.hive_sync.enable: false
      hoodie.datasource.hive_sync.assume_date_partitioning: true
      hoodie.table.name: monthly_account_average_balance_mart
      hoodie.clustering.async.enabled: true
      hoodie.datasource.write.recordkey.field: account_average_balance_skey
      hoodie.datasource.write.partitionpath.field: sys_partition
      hoodie.datasource.write.precombine.field: updated_time
      hoodie.datasource.write.operation: insert
      hoodie.datasource.hive_sync.table: monthly_account_average_balance_mart
