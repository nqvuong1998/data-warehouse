spark:
  spark_conf:
    spark.executor.memory: 2g
    spark.executor.cores: 1
    spark.driver.memory: 1g
    spark.cores.max: 3
    spark.sql.caseSensitive: true
    spark.default.parallelism: 6
    spark.sql.shuffle.partitions: 6
    spark.sql.sources.partitionOverwriteMode: dynamic
    spark.serializer: org.apache.spark.serializer.KryoSerializer
  hadoop_conf:
    fs.s3a.endpoint: https://s3.ap-southeast-1.amazonaws.com

java:
  java_conf:
    user.timezone: Asia/Ho_Chi_Minh

dim:
  surrogate_key_name: address_skey
  business_key_name: address_id
  time_field_name: updated_time

  source:
    format: json
    url: ""
    path: /Users/lap14151/Downloads/Code/data-warehouse/data/lake/addresses/ymd={{yyyyMMdd}}
    schema:
      type: struct
      field:
        address_id: string
        line_1: string
        line_2: string
        district: string
        city: string
        country: string
        updated_time: timestamp

  destination:
    format: org.apache.hudi
    url: ""
    path: /Users/lap14151/Downloads/Code/data-warehouse/data/warehouse/addresses_dim/
    mode: append
    option:
      hoodie.insert.shuffle.parallelism: 2
      hoodie.upsert.shuffle.parallelism: 2
      hoodie.delete.shuffle.parallelism: 2
      hoodie.bulkinsert.shuffle.parallelism: 2
      hoodie.datasource.hive_sync.enable: false
      hoodie.datasource.hive_sync.assume_date_partitioning: true
      hoodie.table.name: addresses_dim
      hoodie.index.type: GLOBAL_BLOOM
      hoodie.bloom.index.update.partition.path: true
      hoodie.clustering.async.enabled: true
      hoodie.datasource.write.recordkey.field: address_skey
      hoodie.datasource.write.partitionpath.field: sys_partition
      hoodie.datasource.write.precombine.field: updated_time
      hoodie.datasource.write.operation: upsert
      hoodie.datasource.hive_sync.table: addresses_dim
