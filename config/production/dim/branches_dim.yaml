spark:
  spark_conf:
    spark.executor.memory: 2g
    spark.executor.cores: 1
    spark.driver.memory: 1g
    spark.cores.max: 3
    spark.sql.caseSensitive: true
    spark.default.parallelism: 6
    spark.sql.shuffle.partitions: 6
    spark.sql.sources.partitionOverwriteMode: dynamic
    spark.serializer: org.apache.spark.serializer.KryoSerializer
  hadoop_conf:
    fs.s3a.endpoint: https://s3.ap-southeast-1.amazonaws.com

java:
  java_conf:
    user.timezone: Asia/Ho_Chi_Minh

dim:
  source:
    branches_log:
      format: json
      url: ""
      path: /Users/lap14151/Downloads/Code/data-warehouse/data/lake/branches/ymd={{yyyyMMdd}}
      schema:
        type: struct
        field:
          branch_id: string
          address_id: string
          branch_type_code: string
          updated_time: timestamp
    branches_dim:
      format: org.apache.hudi
      url: ""
      path: /Users/lap14151/Downloads/Code/data-warehouse/data/warehouse/branches_dim/*
      schema:
        type: struct
        field:
          branch_skey: long
          branch_id: string
          address_skey: long
          branch_type_skey: long
          updated_time: timestamp
          sys_valid_from: timestamp
          sys_valid_to: timestamp
          sys_is_current: boolean
          sys_partition: string
    addresses_dim:
      format: org.apache.hudi
      url: ""
      path: /Users/lap14151/Downloads/Code/data-warehouse/data/warehouse/addresses_dim/*
      schema:
        type: struct
        field:
          address_skey: long
          address_id: string
    branch_types_dim:
      format: org.apache.hudi
      url: ""
      path: /Users/lap14151/Downloads/Code/data-warehouse/data/warehouse/branch_types_dim/*
      schema:
        type: struct
        field:
          branch_type_skey: long
          branch_type_code: string

  destination:
    format: org.apache.hudi
    url: ""
    path: /Users/lap14151/Downloads/Code/data-warehouse/data/warehouse/branches_dim/
    mode: append
    option:
      hoodie.insert.shuffle.parallelism: 2
      hoodie.upsert.shuffle.parallelism: 2
      hoodie.delete.shuffle.parallelism: 2
      hoodie.bulkinsert.shuffle.parallelism: 2
      hoodie.datasource.hive_sync.enable: false
      hoodie.datasource.hive_sync.assume_date_partitioning: true
      hoodie.table.name: branches_dim
      hoodie.index.type: GLOBAL_BLOOM
      hoodie.bloom.index.update.partition.path: true
      hoodie.clustering.async.enabled: true
      hoodie.datasource.write.recordkey.field: branch_skey
      hoodie.datasource.write.partitionpath.field: sys_partition
      hoodie.datasource.write.precombine.field: updated_time
      hoodie.datasource.write.operation: upsert
      hoodie.datasource.hive_sync.table: branches_dim
